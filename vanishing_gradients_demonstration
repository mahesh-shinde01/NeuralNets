# Visualization of loss vs epoch


import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt

# Simple toy dataset
X = torch.randn(100, 2)
y = (X[:, 0] * X[:, 1] > 0).float().unsqueeze(1)  # XOR-like logic

# Define 4 network variants
def get_model(mode):
    layers = []
    if mode == 'normal_relu':
        layers = [nn.Linear(2, 8), nn.ReLU(), nn.Linear(8, 1), nn.Sigmoid()]
    elif mode == 'deep_sigmoid':
        layers = [nn.Linear(2, 8), nn.Sigmoid(),
                  nn.Linear(8, 8), nn.Sigmoid(),
                  nn.Linear(8, 8), nn.Sigmoid(),
                  nn.Linear(8, 1), nn.Sigmoid()]
    elif mode == 'no_activation':
        layers = [nn.Linear(2, 8),  # No activation
                  nn.Linear(8, 1), nn.Sigmoid()]
    elif mode == 'zero_grad':
        layers = [nn.Linear(2, 8), nn.ReLU(), nn.Linear(8, 1), nn.Sigmoid()]
    return nn.Sequential(*layers)

# Training loop
def train(model, mode, epochs=20, lr=0.1):
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    loss_hist = []

    for epoch in range(epochs):
        y_pred = model(X)
        loss = F.mse_loss(y_pred, y)
        loss_hist.append(loss.item())

        optimizer.zero_grad()
        loss.backward()

        # Special case: block gradients
        if mode == 'zero_grad':
            for p in model.parameters():
                p.grad = torch.zeros_like(p.grad)

        optimizer.step()

    return loss_hist

# Run all modes
modes = ['normal_relu', 'deep_sigmoid', 'no_activation', 'zero_grad']
histories = {}
for mode in modes:
    print(f"Training: {mode}")
    model = get_model(mode)
    histories[mode] = train(model, mode)

# Plotting results
plt.figure(figsize=(10, 5))
for mode, loss_list in histories.items():
    plt.plot(loss_list, label=mode.replace('_', ' '))
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("ðŸ“‰ Effect of Activation / Gradient Flow on Learning - trying changing the EPOCH")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

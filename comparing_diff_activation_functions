import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt

# Synthetic XOR-like dataset
torch.manual_seed(42)
X = torch.randn(500, 2)
y = ((X[:, 0] * X[:, 1]) > 0).float().unsqueeze(1)

# Neural network with configurable activation
class SimpleNet(nn.Module):
    def __init__(self, activation_fn):
        super().__init__()
        self.fc1 = nn.Linear(2, 16)
        self.fc2 = nn.Linear(16, 1)
        self.activation_fn = activation_fn

    def forward(self, x):
        z1 = self.fc1(x)
        a1 = self.activation_fn(z1)
        self.last_hidden = a1  # for monitoring
        out = torch.sigmoid(self.fc2(a1))
        return out

# Training function with dead neuron tracking
def train_model(act_name, act_fn, epochs=50, lr=0.1):
    model = SimpleNet(act_fn)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    loss_log = []
    dead_neurons = []

    for epoch in range(epochs):
        y_pred = model(X)
        loss = F.binary_cross_entropy(y_pred, y)
        loss_log.append(loss.item())

        # For ReLU-family, track dead neurons
        if act_name in ["ReLU", "LeakyReLU", "ELU", "GELU"]:
            inactive = (model.last_hidden <= 0).float().mean().item()
        elif act_name == "Sigmoid":
            inactive = (model.last_hidden < 0.1).float().mean().item()
        elif act_name == "Tanh":
            inactive = (torch.abs(model.last_hidden) < 0.1).float().mean().item()
        else:
            inactive = 0

        dead_neurons.append(inactive)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return loss_log, dead_neurons

# Activation functions to compare
activations = {
    "ReLU": nn.ReLU(),
    "LeakyReLU": nn.LeakyReLU(0.01),
    "Tanh": nn.Tanh(),
    "Sigmoid": nn.Sigmoid(),
    "ELU": nn.ELU(),
    "GELU": nn.GELU()
}

# Run experiments
losses = {}
dead_logs = {}
for name, fn in activations.items():
    print(f"Training with {name}...")
    loss, dead = train_model(name, fn)
    losses[name] = loss
    dead_logs[name] = [d * 100 for d in dead]  # convert to %

# Plot Loss Comparison
plt.figure(figsize=(14, 5))
for name in activations:
    plt.plot(losses[name], label=name)
plt.title("ðŸ“‰ Training Loss by Activation Function")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)

# Plot Dead Neuron Percentage
plt.figure(figsize=(14, 5))
for name in activations:
    plt.plot(dead_logs[name], label=name)
plt.title("ðŸ”» % Inactive/Flat Neurons Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Inactive Neurons (%)")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
